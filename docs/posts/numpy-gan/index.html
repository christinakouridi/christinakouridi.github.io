<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Christina Kouridi">
    <meta name="description" content="http://www.example.com">
    <meta name="keywords" content="blog,developer,personal">

    <meta property="og:site_name" content="Christina Kouridi">
    <meta property="og:title" content="
  Vanilla GAN with Numpy - Christina Kouridi
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://www.example.com/posts/numpy-gan/">
    <meta property="og:image" content="http://www.example.com">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="http://www.example.com/posts/numpy-gan/">
    <meta name="twitter:image" content="http://www.example.com">

    <base href="http://www.example.com/posts/numpy-gan/">
    <title>
  Vanilla GAN with Numpy - Christina Kouridi
</title>

    <link rel="canonical" href="http://www.example.com/posts/numpy-gan/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="http://www.example.com/index.xml" type="application/rss+xml" title="Christina Kouridi">
      <link href="http://www.example.com/index.xml" rel="feed" type="application/rss+xml" title="Christina Kouridi" />
    

    <meta name="generator" content="Hugo 0.96.0" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">Christina Kouridi</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://www.example.com/about">About</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://www.example.com/posts">Blog</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="http://www.example.com/research">Research</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://christinakouride.wixsite.com/ckouridi-portfolio">Photography</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="title">Vanilla GAN with Numpy</h1>
      <h2 class="date">July 9, 2019</h2>

      
        <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$','$']],
              displayMath: [['$$','$$']],
              processEscapes: true,
              processEnvironments: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
              TeX: { extensions: ["AMSmath.js", "AMSsymbols.js", "AMSfonts.js"] }
            }
          });
          MathJax.Hub.Queue(function() {
            
            
            
            var all = MathJax.Hub.getAllJax(), i;
            for(i = 0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
          });
          </script>
      
    </header>

    <p>Generative Adversarial Networks (GANs) have achieved tremendous success in generating high-quality synthetic images and efficiently internalising the essence of the images that they learn from. Their potential is enormous, as they can learn to do that for any distribution of data.</p>
<p>In order to keep up with the latest advancements, I decided to explore their theoretical underpinnings by implementing a simple GAN in Python using Numpy. <strong>In this post, I will go through the implementation steps based on Ian Goodfellow’s <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative Adversarial Nets paper</a></strong>.</p>
<p>The full code is available on my <a href="https://github.com/christinakouridi/scratchML/tree/master/GAN">GitHub repository</a>.</p>

<div style="text-align: center;">
<figure style="width:100%;margin-left:auto;margin-right:auto;">
    
        <img src="images/blog_gan_gif.gif"  />
    
    
    <figcaption>
        <i>Figure 1: Sample digit generated by the GAN that we will implement in this tutorial</i>
        
    </figcaption>
    
</figure>
</div>

<h4 id="architecture">Architecture</h4>
<h5 id="gan">GAN</h5>

<div style="text-align: center;">
<figure style="width:80%;margin-left:auto;margin-right:auto;">
    
        <img src="images/blog_gan_2.png"  />
    
    
    <figcaption>
        <i>Figure 2: GAN architecture</i>
        
    </figcaption>
    
</figure>
</div>

<p>We will implement a GAN that generates handwritten digits. The <strong>basic principle of GANs is inspired by the two-player zero-sum game, in which the total gains of two players are zero, and each player’s gain or loss of utility is exactly balanced by the loss or gain of the utility of another player</strong> [1]. It comprises of two models:</p>
<ol>
<li>
<p><strong>Generator</strong>: learns to generate new images, which have a similar data distribution as the real dataset. Crucially, it has no direct access to the real images; it only learns through its interaction with the discriminator.</p>
</li>
<li>
<p><strong>Discriminator</strong>: learns to distinguish candidates produced by the generator from the real data distribution. It outputs the probability that an input image belongs to the real data distribution rather than the generator distribution.</p>
</li>
</ol>
<p>Although both models are typically described by convolutional neural nets, they could be implemented by any form of differentiable system that maps data from one space to another. This implementation uses multilayer preceptors as they are less computationally prohibitive and much easier to code from scratch.</p>
<h5 id="generator">Generator</h5>
<p>The generator, $G$, <strong>is fed random noise</strong>, $z$, from a normal distribution with zero mean and standard deviation of 1 (range [-1,1]). As we will train both the generator and discriminator using mini-batch gradient descent, the input noise will be a numpy array of size <em>[batch size, input layer size]</em>.</p>
<p>The <strong>output of the generator will be a batch of flattened images of size [batch size, image dimension^2]</strong> . The image dimension corresponds to the number of pixels of the training images (for MNIST, each image has size 28×28 pixels).</p>

<div style="text-align: center;">
<figure style="width:30%;margin-left:auto;margin-right:auto;">
    
        <img src="images/blog_gan_3.png"  />
    
    
    <figcaption>
        <i>Figure 3: Generator architecture</i>
        
    </figcaption>
    
</figure>
</div>

<h5 id="discriminator">Discriminator</h5>
<p>The discriminator, <em>D</em>, will be fed a batch of real images from the MNIST dataset and a batch of fake images from the generator. <strong>It will output the probability that the input images are real or fake</strong>.</p>
<p>The original paper suggests training the discriminator for <em>k</em> steps before training the generator for one step. We will choose the least computationally expensive solution, <em>k=1</em>, therefore training the discriminator and generator equally.</p>

<div style="text-align: center;">
<figure style="width:31%;margin-left:auto;margin-right:auto;">
    
        <img src="images/blog_gan_4.png"  />
    
    
    <figcaption>
        <i>Figure 4: Discriminator architecture</i>
        
    </figcaption>
    
</figure>
</div>

<p>Practitioners have been experimenting with the more sophisticated Deep Convolutional GANs to determine the optimal activation functions for the hidden and output layers [2]. I have found their recommendations to be very effective for a simple GAN as well.</p>
<p>In the generator network, it is recommended to use a <em>ReLU</em> activation in the hidden layers and <em>tanh</em> activation in the output layer. It was observed that using a bounded activation allowed the model to learn more quickly to saturate [2]. In the discriminator network, leaky ReLU was found to work well [3,4] in the hidden layers, as it prevents the vanishing gradient problem. At the output layer, a sigmoid activation is commonly used which squeezes pixels that would appear grey toward either black or white, resulting in crisper images. This is in contrast to the original GAN paper, which used the <em>maxout</em> activation.</p>
<h5 id="cost-function">Cost function</h5>
<p><strong>Training of GANs involves balancing two conflicting objectives</strong>:</p>
<ol>
<li>
<p><strong>Training $D$ to maximise the probability of assigning the correct label to the training examples</strong>, $D(x)$ and samples from the generator, $D(G(z))$. The discriminator therefore wants to maximise:</p>
<p>$\begin{aligned}
J^{(D)} = \mathbb{E}_{x \sim p_{data}(x)}logD(x) + \mathbb{E}_{z \sim p_{z}(z)}log(1-D(G(z))
\end{aligned}$</p>
<p>where $p_{data}$ is the training data distribution and $p_{z}$ the noise prior. This is equivalent to minimising:</p>
<p>$\begin{aligned}
J^{(D)} = -\mathbb{E}_{x \sim p_{data}(x)}logD(x) -\mathbb{E}_{z \sim p_{z}(z)}log(1-D(G(z))
\end{aligned}$</p>
<p>This is just the <strong>standard cross-entropy cost that is minimised when training a binary classifier with a sigmoid output</strong>. The only difference is that <strong>the classifier is trained on two mini-batches of data</strong>; one coming from the dataset, where the label is 1 for all examples, and one coming from the generator, where the label is 0 for all examples.</p>
</li>
<li>
<p><strong>Training $G$ to minimise the likelihood of the generated images not coming from the real data distribution</strong>. In other words, $G$ is trying to maximally confuse the discriminator. It tries to minimise:</p>
<p>$\begin{aligned}
J^{(G)} = \mathbb{E}_{z \sim p_{z}(z)}log(1-D(G(z)))
\end{aligned}$</p>
<p>Typically, an alternative, non-saturating training criterion is used for the generator:</p>
<p>$\begin{aligned}
J^{(G)} = -\mathbb{E}_{z \sim p_{z}(z)}log(D(G(z))
\end{aligned}$</p>
</li>
</ol>
<h4 id="implementation">Implementation</h4>
<h5 id="imports">Imports</h5>
<p>Let’s start by importing numpy, <em>matplotlib.pyplot</em> and other useful libraries.</p>
<p><em>Keras.datasets</em> is imported to get access to the MNIST dataset, <em>imageio</em> to generate a gif from sample images at each training iteration and Path to define the location where sample images will be exported that are used to generate the gif.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">imageio</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</span></span></code></pre></div><h5 id="data-loading">Data Loading</h5>
<p>We need a set of real handwritten digits to give the discriminator a starting point in distinguishing between real and fake images. We’ll use <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, a benchmark dataset in deep learning. It consists of 70k images of handwritten digits compiled by the U.S. National Institute of Standards and Technology from Census Bureau employees and high school students.</p>
<p>As we will only use the train data, the test data (10k images) will be ignored.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;y_train.shape&#34;</span><span class="p">,</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;x_train.shape&#34;</span><span class="p">,</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><p>y_train.shape (60000,)<br>
x_train.shape (60000, 28, 28)</p>
<h5 id="initialisation">Initialisation</h5>
<p>We will wrap all functions in the GAN class.</p>
<p>On one GPU, a GAN will need hours to train, and on one CPU more than a day. In addition, GANs are difficult to optimise. For these reasons, <strong>I recommend trying to generate one digit at a time, by limiting the training data from the digits 0-9 to the digit specified in the numbers list</strong>.</p>
<p>We will use a mini-batch size of 64 (batch_size). The input layer of the discriminator is determined by the size of the training images <em>[batch_size, image dimension^2]</em>, which needs to match the output of the generator, i.e. the fake images. The number of neurons at the input layer of the generator (<em>input_layer_size_g</em>) as well as the hidden layers of both models (<em>hidden_layer_size_g, hidden_layer_size_d</em>) need to be defined by us.</p>
<p>Next, to visualise training performance, we can generate a gif of sample images. If <em>create_gif</em> is enabled, a grid of sample images will be saved in your local directory by default and their filename will be stored in the filenames list to enable sourcing the images for stitching at the end of training.</p>
<p>While GANs are commonly used with momentum to adapt the learning rate or the Adam optimiser, we will use a simple step decay quantified by the decay_rate.</p>
<p>Finally, <strong>all weights will be initialised from a zero-centered Normal distribution</strong> with standard deviation determined by the Xavier algorithm[5]. It makes sure the weights are ‘just right’ by keeping the signal in a reasonable range of values through many layers.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GAN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numbers</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_layer_size_g</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">hidden_layer_size_g</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">hidden_layer_size_d</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">decay_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="mi">28</span><span class="p">,</span> <span class="n">display_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">create_gif</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># -------- Initialise hyperparameters --------#</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numbers</span> <span class="o">=</span> <span class="n">numbers</span> <span class="c1"># chosen numbers to be generated</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span> <span class="c1"># #training iterations</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="c1"># #of training examples in each batch</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">nx_g</span> <span class="o">=</span> <span class="n">input_layer_size_g</span> <span class="c1"># #neurons in the generator&#39;s input layer</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">nh_g</span> <span class="o">=</span> <span class="n">hidden_layer_size_g</span> <span class="c1"># #neurons in the generator&#39;s hidden layer</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">nh_d</span> <span class="o">=</span> <span class="n">hidden_layer_size_d</span> <span class="c1"># #neurons in the discriminator&#39;s hidden layer</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="c1"># how much newly acquired info. overrides old info.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dr</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="c1"># learning rate decay after every epoch</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">=</span> <span class="n">image_size</span> <span class="c1"># # pixels of training images</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">display_epochs</span> <span class="o">=</span> <span class="n">display_epochs</span> <span class="c1"># interval for displaying results</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">create_gif</span> <span class="o">=</span> <span class="n">create_gif</span> <span class="c1"># if true, a gif of sample images will be made</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">image_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;./GAN_sample_images&#39;</span><span class="p">)</span> <span class="c1"># new folder in current directory</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_dir</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">image_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">filenames</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># stores filenames of sample images if create_gif is True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># -------- Initialise weights with Xavier method --------#</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># -------- Generator --------#</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W0_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nx_g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh_g</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">                    <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">nx_g</span><span class="p">)</span>  <span class="c1">#100x128</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">b0_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh_g</span><span class="p">))</span>  <span class="c1"># 1x100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W1_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh_g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">                    <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh_g</span><span class="p">)</span> <span class="c1">#128x784</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">b1_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1">#1x784</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># -------- Discriminator --------#</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W0_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh_d</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">                    <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="c1">#784x128</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">b0_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh_d</span><span class="p">))</span>  <span class="c1"># 1x128</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">W1_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh_d</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> \
</span></span><span class="line"><span class="cl">                    <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh_d</span><span class="p">)</span>  <span class="c1"># 128x1</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">b1_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># 1x1</span>
</span></span></code></pre></div><h5 id="data-preprocessing">Data Preprocessing</h5>
<p>Five pre-processing steps were applied to the training data:</p>
<ol>
<li>Limiting it to the subset of digits selected by the user through the numbers list</li>
<li>Removing images that can’t be part of a full training batch</li>
<li>Flattening the images in an array with 784 values representing each pixel’s intensity</li>
<li>Scaling the images the range of the tanh activation function [-1,1]</li>
<li>Shuffling it to enable convergence</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">preprocess_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_train</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_train</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># limit the data to a subset of digits from 0-9</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">numbers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">y_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># limit the data to full batches only</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_batches</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:</span> <span class="n">num_batches</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span> <span class="n">num_batches</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># flatten the images (_,28,28)-&gt;(_, 784)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># normalise the data to the range [-1,1]</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="mf">127.5</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># shuffle the data</span>
</span></span><span class="line"><span class="cl">    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">num_batches</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">preprocess_data</span> <span class="o">=</span> <span class="n">preprocess_data</span>
</span></span></code></pre></div><h5 id="activation-functions">Activation Functions</h5>
<p>Here, we will implement the activation functions that will be used in forward propagation. Numpy’s tanh is used directly. The leaky ReLU function (lrelu) effectively acts as the relu function when the alpha parameter is set to zero.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lrelu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">lrelu</span> <span class="o">=</span> <span class="n">lrelu</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">sigmoid</span>
</span></span></code></pre></div><p>As usual, the derivatives of the activation functions will be needed in backward propagation.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">dlrelu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span>
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">dlrelu</span> <span class="o">=</span> <span class="n">dlrelu</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">dsigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">dsigmoid</span> <span class="o">=</span> <span class="n">dsigmoid</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">dtanh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">dtanh</span> <span class="o">=</span> <span class="n">dtanh</span>
</span></span></code></pre></div><h5 id="forward-propagation">Forward propagation</h5>
<p>Next, we will implement forward propagation for the generator and discriminator network. After the input layer, each layer applies the affine transformation $z = W^{T}x + b$ followed by an activation function $a(z)$.</p>
<p>In the generator, the random noise, $z$, is propagated through the network to produce a batch of fake images, <em>a1_g</em>.</p>
<p>In the discriminator, a batch of images (real or fake), $x$, are propagated through the network to predict a classification (real or fake).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">z0_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W0_g</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0_g</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">a0_g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z0_g</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">z1_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0_g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_g</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1_g</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">a1_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z1_g</span><span class="p">)</span>  <span class="c1"># range [-1,1]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">z1_g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a1_g</span>
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">forward_generator</span> <span class="o">=</span> <span class="n">forward_generator</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward_discriminator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">z0_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W0_d</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b0_d</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">a0_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z0_d</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">z1_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0_d</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_d</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1_d</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">a1_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z1_d</span><span class="p">)</span>  <span class="c1"># output probability [0,1]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">z1_d</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a1_d</span>
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">forward_discriminator</span> <span class="o">=</span> <span class="n">forward_discriminator</span>
</span></span></code></pre></div><h5 id="backward-propagation">Backward propagation</h5>
<p>The GAN setup is reminiscent of reinforcement learning, <strong>where the generator is receiving a reward signal from the discriminator letting it know whether the generated data is accurate or not</strong>. The key difference with GANs, however, is that <strong>we can backward propagate gradient information from the discriminator to the generator</strong>, so the generator knows how to adapt its parameters in order to produce output data that can mislead the discriminator.</p>
<p>We will start by backward propagating the real image gradients through the discriminator and then the fake image gradients through the generator. To do so, we need to pass following information to backward_discriminator:</p>
<ol>
<li><em>x_real</em>: a batch of real images from the training data</li>
<li><em>z1_real</em>: logit output from the discriminator D(x)</li>
<li><em>a1_real</em>: the discriminator’s output predictions for the real images</li>
<li><em>x_fake</em>: a batch with fake images produced by the generator</li>
<li><em>z1_fake</em>: logit output from the discriminator D(G(z))</li>
<li><em>a1_fake</em>: the discriminator’s output predictions for the fake images</li>
</ol>
<p>The gradients are derived by simply differentiating the loss function with respect to each parameter. I will not derive the gradients here as there are many tutorials online, for instance <a href="https://www.youtube.com/watch?v=x_Eamf8MHwU&amp;t=109s">Andrew Ng’s video lectures</a>. For an intuitive understanding of backward propagation, I recommend <a href="https://cs231n.github.io/optimization-2/">Andrej Karpathy’s blog</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">backward_discriminator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_real</span><span class="p">,</span> <span class="n">z1_real</span><span class="p">,</span> <span class="n">a1_real</span><span class="p">,</span> <span class="n">x_fake</span><span class="p">,</span> <span class="n">z1_fake</span><span class="p">,</span> <span class="n">a1_fake</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># -------- Backprop through Discriminator --------#</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># J_D = np.mean(-np.log(a1_real) - np.log(1 - a1_fake))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># real input gradients -np.log(a1_real)</span>
</span></span><span class="line"><span class="cl">    <span class="n">da1_real</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">a1_real</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># 64x1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dz1_real</span> <span class="o">=</span> <span class="n">da1_real</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dsigmoid</span><span class="p">(</span><span class="n">z1_real</span><span class="p">)</span>  <span class="c1"># 64x1</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW1_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0_d</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz1_real</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db1_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz1_real</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">da0_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz1_real</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz0_real</span> <span class="o">=</span> <span class="n">da0_real</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dlrelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z0_d</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW0_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_real</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz0_real</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db0_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz0_real</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># fake input gradients -np.log(1 - a1_fake)</span>
</span></span><span class="line"><span class="cl">    <span class="n">da1_fake</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">a1_fake</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dz1_fake</span> <span class="o">=</span> <span class="n">da1_fake</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dsigmoid</span><span class="p">(</span><span class="n">z1_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW1_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0_d</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz1_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db1_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz1_fake</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">da0_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz1_fake</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz0_fake</span> <span class="o">=</span> <span class="n">da0_fake</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dlrelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z0_d</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW0_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_fake</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz0_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db0_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz0_fake</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># -------- Combine gradients for real &amp; fake images--------#</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW1</span> <span class="o">=</span> <span class="n">dW1_real</span> <span class="o">+</span> <span class="n">dW1_fake</span>
</span></span><span class="line"><span class="cl">    <span class="n">db1</span> <span class="o">=</span> <span class="n">db1_real</span> <span class="o">+</span> <span class="n">db1_fake</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dW0</span> <span class="o">=</span> <span class="n">dW0_real</span> <span class="o">+</span> <span class="n">dW0_fake</span>
</span></span><span class="line"><span class="cl">    <span class="n">db0</span> <span class="o">=</span> <span class="n">db0_real</span> <span class="o">+</span> <span class="n">db0_fake</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># -------- Update gradients using SGD--------#</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">W0_d</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dW0</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">b0_d</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">W1_d</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dW1</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">b1_d</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db1</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">backward_discriminator</span> <span class="o">=</span> <span class="n">backward_discriminator</span>
</span></span></code></pre></div><p>In <em>backward_generator</em>, we will calculate the gradients at the beginning and end of the discriminator but we won’t update the discriminator weights.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">backward_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x_fake</span><span class="p">,</span> <span class="n">z1_fake</span><span class="p">,</span> <span class="n">a1_fake</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># -------- Backprop through Discriminator --------#</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># J_D = np.mean(-np.log(a1_real) - np.log(1 - a1_fake))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># fake input gradients -np.log(1 - a1_fake)</span>
</span></span><span class="line"><span class="cl">    <span class="n">da1_d</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">a1_fake</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># 64x1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dz1_d</span> <span class="o">=</span> <span class="n">da1_d</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dsigmoid</span><span class="p">(</span><span class="n">z1_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">da0_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz1_d</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz0_d</span> <span class="o">=</span> <span class="n">da0_d</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dlrelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z0_d</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz0_d</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W0_d</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># -------- Backprop through Generator --------#</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># J_G = np.mean(-np.log(1 - a1_fake))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz1_g</span> <span class="o">=</span> <span class="n">dx_d</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z1_g</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW1_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a0_g</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz1_g</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db1_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz1_g</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">da0_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz1_g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1_g</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dz0_g</span> <span class="o">=</span> <span class="n">da0_g</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dlrelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z0_g</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW0_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz0_g</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db0_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz0_g</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># -------- Update gradients using SGD --------#</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">W0_g</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dW0_g</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">b0_g</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db0_g</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">W1_g</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dW1_g</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">b1_g</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db1_g</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">backward_generator</span> <span class="o">=</span> <span class="n">backward_generator</span>
</span></span></code></pre></div><h5 id="sampling--gif-generation">Sampling &amp; GIF generation</h5>
<p>Sample_images will enable us to view digits from the generator’s distribution at the frequency defined by the user through the display_epoch hyperparameter. After training each batch, we will generate a grid of sample images (but not show it if the frequency criterion is not met) and save it in the <em>GAN_sample_images</em> folder in your current directory.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample_images</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">show</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mf">127.5</span> <span class="o">+</span> <span class="mf">127.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># saves generated images in the GAN_sample_images folder</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_gif</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_epoch_filename</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_dir</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;GAN_epoch</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">.png&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">filenames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_epoch_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">current_epoch_filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">show</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">sample_images</span> <span class="o">=</span> <span class="n">sample_images</span>
</span></span></code></pre></div><p>At the end of training, we will generate a gif from the sample images of the generator if create_gif is initialised to True. This can be achieved with imageio in a few lines of code, which can read from filenames, file objects, http, zipfiles and bytes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_gif</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">filenames</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">imageio</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">imageio</span><span class="o">.</span><span class="n">mimsave</span><span class="p">(</span><span class="s2">&#34;GAN.gif&#34;</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">generate_gif</span> <span class="o">=</span> <span class="n">generate_gif</span>
</span></span></code></pre></div><h5 id="training">Training</h5>
<p>Finally, we will define a function to train the model. train takes as input raw images and labels and outputs the loss of the Generator and Discriminator at each training step.</p>
<p>In order to speed up training, we will train our data in batches. The number of batches (<em>num_batches</em>) is determined by the total number of training images divided by the user-defined <em>batch_size</em>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">J_Ds</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># stores the disciminator losses</span>
</span></span><span class="line"><span class="cl">    <span class="n">J_Gs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># stores the generator losses</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># preprocess input; note that the labels aren&#39;t needed</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_train</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">num_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ------- PREPARE INPUT BATCHES &amp; NOISE -------#</span>
</span></span><span class="line"><span class="cl">            <span class="n">x_real</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]</span> <span class="c1"># 64x784</span>
</span></span><span class="line"><span class="cl">            <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nx_g</span><span class="p">])</span>  <span class="c1"># 64x100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># ------- FORWARD PROPAGATION -------#</span>
</span></span><span class="line"><span class="cl">            <span class="n">z1_g</span><span class="p">,</span> <span class="n">x_fake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_generator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">z1_d_real</span><span class="p">,</span> <span class="n">a1_d_real</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_discriminator</span><span class="p">(</span><span class="n">x_real</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">z1_d_fake</span><span class="p">,</span> <span class="n">a1_d_fake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_discriminator</span><span class="p">(</span><span class="n">x_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># ------- CROSS ENTROPY LOSS -------#</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ver1 : max log(D(x)) + log(1 - D(G(z))) (in original paper)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ver2 : min -log(D(x)) min -log(1 - D(G(z))) (implemented here)</span>
</span></span><span class="line"><span class="cl">            <span class="n">J_D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a1_d_real</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a1_d_fake</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">J_Ds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J_D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># ver1 : minimize log(1 - D(G(z))) (in original paper)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ver2 : maximize log(D(G(z)))</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ver3 : minimize -log(D(G(z))) (implemented here)</span>
</span></span><span class="line"><span class="cl">            <span class="n">J_G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a1_d_fake</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">J_Gs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J_G</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ------- BACKWARD PROPAGATION -------#</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">backward_discriminator</span><span class="p">(</span><span class="n">x_real</span><span class="p">,</span> <span class="n">z1_d_real</span><span class="p">,</span> <span class="n">a1_d_real</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">x_fake</span><span class="p">,</span> <span class="n">z1_d_fake</span><span class="p">,</span> <span class="n">a1_d_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">backward_generator</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x_fake</span><span class="p">,</span> <span class="n">z1_d_fake</span><span class="p">,</span> <span class="n">a1_d_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">display_epochs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Epoch:</span><span class="si">{</span><span class="n">epoch</span><span class="si">:}</span><span class="s2">|G loss:</span><span class="si">{</span><span class="n">J_G</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">|D loss:</span><span class="si">{</span><span class="n">J_D</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">|D(G(z))avg:</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a1_d_fake</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">|D(x)avg:</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a1_d_real</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">|LR:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">sample_images</span><span class="p">(</span><span class="n">x_fake</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># display sample images</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">sample_images</span><span class="p">(</span><span class="n">x_fake</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># reduce learning rate after every epoch</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dr</span> <span class="o">*</span> <span class="n">epoch</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># generate gif</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_gif</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">generate_gif</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">J_Ds</span><span class="p">,</span> <span class="n">J_Gs</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">GAN</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train</span>
</span></span></code></pre></div><p>We can now train our GAN by alternating the training of the discriminator and the generator. As discussed earlier, to get quick results, I recommend running the model for one digit only, which is defined in the numbers list.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">numbers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">GAN</span><span class="p">(</span><span class="n">numbers</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">J_Ds</span><span class="p">,</span> <span class="n">J_Gs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div><p>The next figure visualises the loss of the discriminator and generator at each training step. As training progresses the generator error decreases, implying that the images it generates are improving. While the generator improves, the discriminator’s error increases, because the synthetic images are becoming more realistic each time.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">J_Ds</span><span class="p">))],</span> <span class="n">J_Ds</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">J_Gs</span><span class="p">))],</span> <span class="n">J_Gs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;# training steps&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;training cost&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Discriminator&#39;</span><span class="p">,</span> <span class="s1">&#39;Generator&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div>
<div style="text-align: center;">
<figure style="width:40%;margin-left:auto;margin-right:auto;">
    
        <img src="images/blog_gan_5.png"  />
    
    
    <figcaption>
        <i>Figure 5: Evolution of the discriminator and generator training losses</i>
        
    </figcaption>
    
</figure>
</div>

<h4 id="remarks">Remarks</h4>
<p>GANs are known to be difficult to optimise. Without the right network architecture, hyperparameters, and training procedure, the discriminator can overpower the generator, or vice-versa. You can experience this yourself by trying to optimise the GAN implemented in this tutorial for all digits (0-9). The two most common failure modes are:</p>
<ol>
<li><strong>The generator overpowers the discriminator (mode collapse)</strong>. The generator can collapse to a parameter setting where it <strong>always emits the same samples that the discriminator believes are highly realistic</strong>. You can recognise mode collapse in your GAN if it generates very similar images. Mode collapse can sometimes be corrected by strengthening the discriminator in some way—for instance, by adjusting its learning rate or by reconfiguring its layers.</li>
<li><strong>The discriminator overpowers the generator, classifying generated images as fake with absolute certainty</strong>. When the discriminator responds with absolute certainty, it leaves no gradient for the generator to descend.</li>
</ol>
<p>Practitioners have amassed many strategies to mitigate these instabilities and improve the performance of GANs [5, 6] . A summary of key strategies can be found at <a href="https://github.com/soumith/ganhacks">this GitHub repository</a>. These should be regarded as techniques that are worth trying out, not as best practices. As implementing and testing these techniques with Numpy would be extremely time-consuming, I recommend using a deep learning library like TensorFlow. You can find my improved version of a GAN, implemented with TensorFlow 2.0, in my <a href="https://github.com/christinakouridi/TensorFlow2.0/tree/master/VanillaGAN">GitHub repository</a>.</p>
<h4 id="references">References</h4>
<p>[1] Wang Kunfeng, Gou Chao, Duan Yanjie, Lin Yilun, Zheng Xinhu and Wang Fei-Yue. (2017). “Generative Adversarial  Networks: Introduction and Outlook”.</p>
<p>[2] Radford Alec, Metz Luke and Chintala Soumith. (2015). “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”.</p>
<p>[3] Maas Andrew L, Hannun Awni Y, and Ng Andrew Y. (2013). “Rectifier nonlinearities improve neural network acoustic models”.</p>
<p>[4] Xu Bing, Wang Naiyan, Chen Tianqi, and Li Mu. (2015). “Empirical evaluation of rectified activations in convolutional network”.</p>
<p>[5] Glorot Xavier and Bengio Yoshua. (2010). “Understanding the difficulty of training deep feedforward neural networks”.</p>
<p>[6] Salimans Tim, Goodfellow Ian, Zaremba Wojciech, Cheung Vicki, Radford Alec and Chen Xi. (2016). “Improved techniques for training GANs”.</p>

  </article>

  <br/>

  
  
</section>

      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
        
        
        
        
      </div>
    
    
     © 2021    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 

  </section>
</footer>
<div>
  <section class="container">
    
    
      <div class="sns-shares pc-sns-shares">
        
        
        
        
        
      </div>
    
  </section>
</div>

      
    </main>

    

  <script src="/js/app.js"></script>
  
  </body>
</html>
